{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6115123,"sourceType":"datasetVersion","datasetId":3504592}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/input/chemcancer-v2/src/\n%mkdir /kaggle/working/CV_BLS_fold_data\n%mkdir /kaggle/working/CV_BLS_cm","metadata":{"papermill":{"duration":0.028008,"end_time":"2023-07-11T06:03:12.621625","exception":false,"start_time":"2023-07-11T06:03:12.593617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-25T16:30:20.265773Z","iopub.execute_input":"2023-12-25T16:30:20.266712Z","iopub.status.idle":"2023-12-25T16:30:22.553352Z","shell.execute_reply.started":"2023-12-25T16:30:20.266669Z","shell.execute_reply":"2023-12-25T16:30:22.551516Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/chemcancer-v2/src\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install rvfln","metadata":{"papermill":{"duration":14.221733,"end_time":"2023-07-11T06:03:26.848751","exception":false,"start_time":"2023-07-11T06:03:12.627018","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-25T16:30:23.201023Z","iopub.execute_input":"2023-12-25T16:30:23.201585Z","iopub.status.idle":"2023-12-25T16:30:42.610226Z","shell.execute_reply.started":"2023-12-25T16:30:23.201537Z","shell.execute_reply":"2023-12-25T16:30:42.608754Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting rvfln\n  Downloading rvfln-0.0.6.tar.gz (3.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: rvfln\n  Building wheel for rvfln (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rvfln: filename=rvfln-0.0.6-py3-none-any.whl size=5114 sha256=298b6019bd575b62e878c2352db2e5fc8ebf89097321112494e33576f2301208\n  Stored in directory: /root/.cache/pip/wheels/b3/d7/24/732c199b48ade58f8add6a6273be34bb89a52088c38e4864af\nSuccessfully built rvfln\nInstalling collected packages: rvfln\nSuccessfully installed rvfln-0.0.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport time\nfrom tensorflow.keras.optimizers import Adam\nfrom data import *\nfrom machine_learning_models import *\nfrom deep_learning_models import *\nfrom vision_transformer import *\nfrom utils_dl_model import *\nfrom utils_ml_model import print_ml_results\nfrom sklearn.model_selection import train_test_split\nfrom rvfln.bls import BLSClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":8.891479,"end_time":"2023-07-11T06:03:35.746429","exception":false,"start_time":"2023-07-11T06:03:26.85495","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-25T16:30:42.612824Z","iopub.execute_input":"2023-12-25T16:30:42.613217Z","iopub.status.idle":"2023-12-25T16:30:57.590149Z","shell.execute_reply.started":"2023-12-25T16:30:42.613181Z","shell.execute_reply":"2023-12-25T16:30:57.588311Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the seed value.\nSEED = 7\nnp.random.seed(SEED)\n\n# Deep Learning parameters\nDL_EPOCH = 500\nDL_BATCH_SIZE = 32\nDL_CNN_VERSION = 3\nDL_TRANSFORMER_VISION_VERSION = 11\nDL_BLS_VERSION = 1\n\nDO_DL = True\nCV_DL = True\nOPT_DL = False\n\nDO_CNN = False\nDO_TRANSFORMER_VISION = False\nDO_BLS = True\nDO_ML = False\n\n# Percentage of test set out of the dataset.\nTEST_SET = 0.2\n\n# Percentage of validation set out of the training dataset.\nVAL_SET = 0.2","metadata":{"papermill":{"duration":0.016798,"end_time":"2023-07-11T06:03:35.769801","exception":false,"start_time":"2023-07-11T06:03:35.753003","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-25T16:30:57.591916Z","iopub.execute_input":"2023-12-25T16:30:57.592702Z","iopub.status.idle":"2023-12-25T16:30:57.601744Z","shell.execute_reply.started":"2023-12-25T16:30:57.592663Z","shell.execute_reply":"2023-12-25T16:30:57.599414Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Folder path associated with deep learning models\ndl_models_folder = \"/kaggle/working/Deep_Learning_models/\"\ndl_metrics_folder = \"/kaggle/working/Deep_Learning_metrics/\"\ndl_weights_folder = \"/kaggle/working/Deep_Learning_weights/\"\ndl_cv_models_folder = \"/kaggle/working/Deep_Learning_CV/\"\ndl_cv_results_folder = \"/kaggle/working/Deep_Learning_CV_results/\"\n\n# Folder path associated with machine learning models\nml_models_folder = \"/kaggle/working/Machine_Learning_models/\"\nml_models_results_folder = \"/kaggle/working/Machine_Learning_models_results/\"\n\n# Model names (Saved in h5 format)\ncnn_model_name = f\"cnn_v{DL_CNN_VERSION}_{DL_BATCH_SIZE}_{DL_EPOCH}_seed_{SEED}.h5\"\ntransformer_vis_model_name = f\"transformer_vision_v{DL_TRANSFORMER_VISION_VERSION}_{DL_BATCH_SIZE}_{DL_EPOCH}_seed_{SEED}.h5\"\nbls_model_name = f\"bls_v{DL_BLS_VERSION}_{DL_BATCH_SIZE}_{DL_EPOCH}_seed_{SEED}.h5\"\n\n# Metric filenames\ncnn_metrics_filename = f\"metrics_{cnn_model_name}.json\"\ntransformer_vis_metrics_filename = f\"metrics_{transformer_vis_model_name}.json\"\n\n# Weight filenames\ncnn_weights_filename = f\"weights_{cnn_model_name}.json\"\ntransformer_vis_weights_filename = f\"weights_{transformer_vis_model_name}.json\"\n\n# Deep Learning models path\nif DO_CNN:\n    dl_model_path = os.path.join(\n        dl_models_folder, cnn_model_name)\nelif DO_TRANSFORMER_VISION:\n    dl_model_path = os.path.join(\n        dl_models_folder, transformer_vis_model_name)\n\n\n# Deep Learning metrics path\nif DO_CNN:\n    dl_metrics_path = os.path.join(\n        dl_metrics_folder, cnn_metrics_filename)\nelif DO_TRANSFORMER_VISION:\n    dl_metrics_path = os.path.join(\n        dl_metrics_folder, transformer_vis_metrics_filename)\n\n\n# Deep Learning weights path\nif DO_CNN:\n    dl_weights_path = os.path.join(\n        dl_weights_folder, cnn_weights_filename)\n\nelif DO_TRANSFORMER_VISION:\n    dl_weights_path = os.path.join(\n        dl_weights_folder, transformer_vis_weights_filename)","metadata":{"papermill":{"duration":0.017912,"end_time":"2023-07-11T06:03:35.79377","exception":false,"start_time":"2023-07-11T06:03:35.775858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-25T16:31:07.105038Z","iopub.execute_input":"2023-12-25T16:31:07.105540Z","iopub.status.idle":"2023-12-25T16:31:07.117902Z","shell.execute_reply.started":"2023-12-25T16:31:07.105499Z","shell.execute_reply":"2023-12-25T16:31:07.116669Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load the data\ndata_file = \"../Data/HC05_HC07.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-12-25T16:31:09.564287Z","iopub.execute_input":"2023-12-25T16:31:09.565093Z","iopub.status.idle":"2023-12-25T16:31:09.571512Z","shell.execute_reply.started":"2023-12-25T16:31:09.565020Z","shell.execute_reply":"2023-12-25T16:31:09.570006Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load the data\nprint(\"Load the data\")\ndata = load_data(data_file)\nprint(f\"Data shape : {data.shape}\")\n\n# Extract the feature and target data\nprint(\"Extract the feature and target data\")\nX, y = extract_data(data)\nprint(f\"X shaped: {X.shape}\")\nprint(f\"y shaped: {y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T16:31:10.843959Z","iopub.execute_input":"2023-12-25T16:31:10.844515Z","iopub.status.idle":"2023-12-25T16:31:11.146232Z","shell.execute_reply.started":"2023-12-25T16:31:10.844474Z","shell.execute_reply":"2023-12-25T16:31:11.144582Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Load the data\nData shape : (2373, 274)\nExtract the feature and target data\nX shaped: (2373, 270)\ny shaped: (2373,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom rvfln.bls import BLSClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n\ndef perform_bls_cv(X,y,n_z, n_z_features, n_h, alpha):\n\n    bls = BLSClassifier(n_z=n_z, n_z_features=n_z_features, n_h=n_h, alpha=alpha)\n\n    fold = 1\n    n_splits = 5\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed_value)\n    scores = []\n    # Initialize lists to store precision, recall, and F1 scores\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n    confusion_matrices_bls = []\n\n\n    for train_index, test_index in skf.split(X, y):\n        # Split the data into training and validation sets\n        print(\"=\" * 40)\n        print(f\"Fold: {fold}\")\n        print(\"Splitting the data\")\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        # Standardize the data\n        print(\"Standardizing the data\")\n        X_train, X_test = standardize_data(X_train, X_test)\n        \n        print(f\"X_train shape: {X_train.shape}\")\n        print(f\"X_test shape: {X_test.shape}\")\n        print(f\"y_train shape: {y_train.shape}\")\n        print(f\"y_test shape: {y_test.shape}\")\n        \n        print(\"Saving the data\")\n        fold_data = {\n\n            'X_train': X_train,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_test': y_test\n        }\n\n        with open(f\"/kaggle/working/CV_BLS_fold_data/fold_{fold}_data.pkl\", 'wb') as f:\n            pickle.dump(fold_data, f)\n        \n        # Fit the bls and compute the accuracy\n        bls.fit(X_train, y_train)\n        score = bls.score(X_test, y_test)\n        y_pred = bls.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        \n        # Calculate precision, recall, and F1 score\n        precision = precision_score(y_test, y_pred, average='macro')\n        recall = recall_score(y_test, y_pred, average='macro')\n        f1 = f1_score(y_test, y_pred, average='macro')\n        cm = confusion_matrix(y_test, y_pred)\n        \n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n        \n        with open(f\"/kaggle/working/CV_BLS_cm/BLS_cm_fold_{fold}.pkl\", 'wb') as cm_file:\n            pickle.dump(cm, cm_file)\n\n        # Append the scores to the lists\n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n        scores.append(score)\n        confusion_matrices_bls.append(cm)\n        \n        \n        \n        print(f\"Fold {fold}, BLS Test Score: {score:.4f}\")\n        fold += 1\n    \n    scores = np.array(scores)\n    accuracies = np.array(accuracies)\n    precisions = np.array(precisions)\n    recalls = np.array(recalls)\n    f1_scores = np.array(f1_scores)\n    \n    # Calculate and print the mean and standard deviation of precision, recall, and F1 score\n    print(f\"Mean BLS Test score over stratified {n_splits}-fold cross-validation: {np.mean(scores):.4f}\")\n    print(f\"Mean accuracies over stratified {n_splits}-fold cross-validation: {np.mean(accuracies):.4f}\")\n    print(f\"Mean precision over stratified {n_splits}-fold cross-validation: {np.mean(precisions):.4f}\")\n    print(f\"Mean recall over stratified {n_splits}-fold cross-validation: {np.mean(recalls):.4f}\")\n    print(f\"Mean F1 score over stratified {n_splits}-fold cross-validation: {np.mean(f1_scores):.4f}\")\n    print(\"*\" * 40)\n    print(f\"Standard deviation of BLS Test score over stratified {n_splits}-fold cross-validation: {np.std(scores):.4f}\")\n    print(f\"Standard deviation of accuracies over stratified {n_splits}-fold cross-validation: {np.std(accuracies):.4f}\")\n    print(f\"Standard deviation of precision over stratified {n_splits}-fold cross-validation: {np.std(precisions):.4f}\")\n    print(f\"Standard deviation of recall over stratified {n_splits}-fold cross-validation: {np.std(recalls):.4f}\")\n    print(f\"Standard deviation of F1 score over stratified {n_splits}-fold cross-validation: {np.std(f1_scores):.4f}\")\n    print(confusion_matrices_bls)\n    \n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T16:31:13.215979Z","iopub.execute_input":"2023-12-25T16:31:13.216548Z","iopub.status.idle":"2023-12-25T16:31:13.241609Z","shell.execute_reply.started":"2023-12-25T16:31:13.216504Z","shell.execute_reply":"2023-12-25T16:31:13.239790Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"n_z=12\nn_z_features=50\nn_h=100\nalpha=0.0001\n\nperform_bls_cv(X,y,n_z, n_z_features, n_h, alpha)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T16:31:57.434999Z","iopub.execute_input":"2023-12-25T16:31:57.436147Z","iopub.status.idle":"2023-12-25T16:31:59.097727Z","shell.execute_reply.started":"2023-12-25T16:31:57.436094Z","shell.execute_reply":"2023-12-25T16:31:59.096179Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"========================================\nFold: 1\nSplitting the data\nStandardizing the data\nX_train shape: (1898, 270)\nX_test shape: (475, 270)\ny_train shape: (1898,)\ny_test shape: (475,)\nSaving the data\nPrecision: 0.7337\nRecall: 0.6997\nF1 Score: 0.7027\nFold 1, BLS Test Score: 0.7158\n========================================\nFold: 2\nSplitting the data\nStandardizing the data\nX_train shape: (1898, 270)\nX_test shape: (475, 270)\ny_train shape: (1898,)\ny_test shape: (475,)\nSaving the data\nPrecision: 0.7151\nRecall: 0.6903\nF1 Score: 0.6931\nFold 2, BLS Test Score: 0.7053\n========================================\nFold: 3\nSplitting the data\nStandardizing the data\nX_train shape: (1898, 270)\nX_test shape: (475, 270)\ny_train shape: (1898,)\ny_test shape: (475,)\nSaving the data\nPrecision: 0.7413\nRecall: 0.6918\nF1 Score: 0.6953\nFold 3, BLS Test Score: 0.7095\n========================================\nFold: 4\nSplitting the data\nStandardizing the data\nX_train shape: (1899, 270)\nX_test shape: (474, 270)\ny_train shape: (1899,)\ny_test shape: (474,)\nSaving the data\nPrecision: 0.6744\nRecall: 0.6563\nF1 Score: 0.6555\nFold 4, BLS Test Score: 0.6751\n========================================\nFold: 5\nSplitting the data\nStandardizing the data\nX_train shape: (1899, 270)\nX_test shape: (474, 270)\ny_train shape: (1899,)\ny_test shape: (474,)\nSaving the data\nPrecision: 0.7174\nRecall: 0.6939\nF1 Score: 0.6952\nFold 5, BLS Test Score: 0.7110\nMean BLS Test score over stratified 5-fold cross-validation: 0.7033\nMean accuracies over stratified 5-fold cross-validation: 0.7033\nMean precision over stratified 5-fold cross-validation: 0.7164\nMean recall over stratified 5-fold cross-validation: 0.6864\nMean F1 score over stratified 5-fold cross-validation: 0.6884\n****************************************\nStandard deviation of BLS Test score over stratified 5-fold cross-validation: 0.0145\nStandard deviation of accuracies over stratified 5-fold cross-validation: 0.0145\nStandard deviation of precision over stratified 5-fold cross-validation: 0.0232\nStandard deviation of recall over stratified 5-fold cross-validation: 0.0154\nStandard deviation of F1 score over stratified 5-fold cross-validation: 0.0168\n[array([[167,   6,  10],\n       [ 42,  88,  12],\n       [ 39,  26,  85]]), array([[162,   9,  12],\n       [ 33,  86,  23],\n       [ 44,  19,  87]]), array([[170,   7,   6],\n       [ 43,  88,  11],\n       [ 49,  22,  79]]), array([[163,  11,   9],\n       [ 41,  74,  26],\n       [ 37,  30,  83]]), array([[167,   6,  10],\n       [ 37,  84,  20],\n       [ 37,  27,  86]])]\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.7033204530313125"},"metadata":{}}]},{"cell_type":"markdown","source":"n_z is the number of feature nodes in the enhancement node layer. It should ideally be set considering the dimensionality of your input data. A larger n_z means the model can learn more complex representations, but it may also lead to overfitting if set too high. For your data with 270 features, a reasonable starting point might be around 10-50.\n\nn_z_features is the number of random features generated from each feature node. This parameter directly controls the model's complexity and its computational requirements. A higher number will allow the model to learn more complex representations, but it will also increase the computational cost and may lead to overfitting. A reasonable starting point might be 100-500.\n\nn_h is the number of enhancement nodes. This is another parameter controlling the model's complexity. A higher number will allow the model to learn more complex representations, but it will also increase the computational cost and may lead to overfitting. A reasonable starting point might be 500-2000.\n\nalpha is a regularization parameter. It helps prevent overfitting by adding a penalty to the loss function based on the weights' magnitude. It is typically a small positive value. Common choices are 0.1, 0.01, or 0.001.","metadata":{"papermill":{"duration":0.058055,"end_time":"2023-07-11T06:13:10.091683","exception":false,"start_time":"2023-07-11T06:13:10.033628","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Best parameters:  {'alpha': 0.001, 'n_h': 50, 'n_z': 10, 'n_z_features': 50}\nBest score:  0.7470962366338009","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}}]}