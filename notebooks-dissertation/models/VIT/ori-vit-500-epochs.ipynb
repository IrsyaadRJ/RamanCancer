{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6115123,"sourceType":"datasetVersion","datasetId":3504592}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/input/chemcancer-v2/src/\n%mkdir /kaggle/working/Deep_Learning_metrics/\n%mkdir /kaggle/working/During_train/\n%mkdir /kaggle/working/CV_VIT_models\n%mkdir /kaggle/working/CV_VIT_results\n%mkdir /kaggle/working/CV_fold_data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-05T20:18:42.682446Z","iopub.execute_input":"2023-10-05T20:18:42.682821Z","iopub.status.idle":"2023-10-05T20:18:47.844522Z","shell.execute_reply.started":"2023-10-05T20:18:42.682781Z","shell.execute_reply":"2023-10-05T20:18:47.843420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport time\nfrom tensorflow.keras.optimizers import Adam\nfrom data import *\nfrom machine_learning_models import *\nfrom deep_learning_models import *\nfrom vision_transformer import *\nfrom utils_dl_model import *\nfrom utils_ml_model import print_ml_results\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2023-10-05T20:18:47.846933Z","iopub.execute_input":"2023-10-05T20:18:47.847497Z","iopub.status.idle":"2023-10-05T20:18:49.845389Z","shell.execute_reply.started":"2023-10-05T20:18:47.847460Z","shell.execute_reply":"2023-10-05T20:18:49.844477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the seed value.\nSEED = 7\nnp.random.seed(SEED)\n\n# Deep Learning parameters\nDL_EPOCH = 500\nDL_BATCH_SIZE = 32\nDL_CNN_VERSION = 3\nDL_TRANSFORMER_VISION_VERSION = 14\nDL_CV_FOLD = 5\n\nDO_DL = True\nCV_DL = True\nOPT_DL = False\n\nDO_CNN = False\nDO_TRANSFORMER_VISION = True\nDO_ML = False\n\n# Percentage of test set out of the dataset.\nTEST_SET = 0.2\n\n# Percentage of validation set out of the training dataset.\nVAL_SET = 0.2\n\n# Folder path associated with deep learning models\ndl_models_folder = \"../Deep_Learning_models/\"\ndl_metrics_folder = \"../Deep_Learning_metrics/\"\ndl_weights_folder = \"../Deep_Learning_weights/\"\ndl_cv_models_folder = \"../Deep_Learning_CV/\"\ndl_cv_results_folder = \"../Deep_Learning_CV_results/\"\n\n# Folder path associated with machine learning models\nml_models_folder = \"../Machine_Learning_models/\"\nml_models_results_folder = \"../Machine_Learning_models_results/\"\n\n# Model names (Saved in h5 format)\ncnn_model_name = f\"cnn_v{DL_CNN_VERSION}_{DL_BATCH_SIZE}_{DL_EPOCH}_seed_{SEED}.h5\"\ntransformer_vis_model_name = f\"transformer_vision_v{DL_TRANSFORMER_VISION_VERSION}_{DL_BATCH_SIZE}_{DL_EPOCH}_seed_{SEED}.h5\"\n\n# Metric filenames\ncnn_metrics_filename = f\"metrics_{cnn_model_name}.json\"\ntransformer_vis_metrics_filename = f\"metrics_{transformer_vis_model_name}.json\"\n\n# Weight filenames\ncnn_weights_filename = f\"weights_{cnn_model_name}.json\"\ntransformer_vis_weights_filename = f\"weights_{transformer_vis_model_name}.json\"\n\n\n# Deep Learning models path\nif DO_CNN:\n    dl_model_path = os.path.join(\n        dl_models_folder, cnn_model_name)\nelif DO_TRANSFORMER_VISION:\n    dl_model_path = os.path.join(\n        dl_models_folder, transformer_vis_model_name)\n\n\n# Deep Learning metrics path\nif DO_CNN:\n    dl_metrics_path = os.path.join(\n        dl_metrics_folder, cnn_metrics_filename)\nelif DO_TRANSFORMER_VISION:\n    dl_metrics_path = os.path.join(\n        dl_metrics_folder, transformer_vis_metrics_filename)\n\n\n# Deep Learning weights path\nif DO_CNN:\n    dl_weights_path = os.path.join(\n        dl_weights_folder, cnn_weights_filename)\n\nelif DO_TRANSFORMER_VISION:\n    dl_weights_path = os.path.join(\n        dl_weights_folder, transformer_vis_weights_filename)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T20:18:50.789517Z","iopub.execute_input":"2023-10-05T20:18:50.790167Z","iopub.status.idle":"2023-10-05T20:18:50.800051Z","shell.execute_reply.started":"2023-10-05T20:18:50.790137Z","shell.execute_reply":"2023-10-05T20:18:50.798620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\ndef perform_vit_cross_validation(X, y, patch_size, embed_dim, num_heads, mlp_dim, num_layers, dropout_rate,\n                                 n_splits=5, epochs=100, batch_size=32, vit_version = 1, val_set = 0.2,\n                                 do_opt = False,models_folder = \"models\", results_folder = \"results\"):\n    \n    if not os.path.exists(models_folder):\n        os.makedirs(models_folder)\n\n    if not os.path.exists(results_folder):\n        os.makedirs(results_folder)\n    accuracies = []\n    test_losses = []\n    test_accuracies = []\n    \n    # Initialize lists to store precision, recall, and F1 scores\n    precisions = []\n    recalls = []\n    f1_scores = []\n    \n    # Initialize lists to store losses and accuracies for each epoch\n    mean_training_losses = []\n    mean_training_accuracies = []\n    mean_val_losses = []   \n    mean_val_accuracies = []\n    \n    fold = 1\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed_value)\n    \n    for train_index, test_index in skf.split(X, y):\n        # Split the data into training and validation sets\n        print(\"=\" * 40)\n        print(f\"Fold: {fold}\")\n        print(\"Splitting the data\")\n        X_train_full, X_test = X[train_index], X[test_index]\n        y_train_full, y_test = y[train_index], y[test_index]\n        \n        # Standardize the data\n        print(\"Standardizing the data\")\n        X_train_full, X_test = standardize_data(X_train_full, X_test)\n        \n        # Reshape the data\n        print(\"Reshaping the data for deep learning models\")\n        X_train_full, X_test = reshape_data(X_train_full, X_test)\n        \n        # Input shape and number of classes for the model\n        input_shape = (X_train_full.shape[1], 1)\n        num_classes = len(np.unique(y))\n        \n        X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size = 0.2, random_state = seed_value, stratify = y_train_full)\n        num_patches = input_shape[0] // patch_size\n        try:\n            if do_opt:\n                # Create the model\n                model = create_vit(input_shape, patch_size, num_patches, num_classes,\n                                embed_dim, num_heads, mlp_dim, num_layers, dropout_rate)\n                model.summary()\n                # Compile the model\n                model.compile(\n                    optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n            else:\n                # Create the model\n                print(\"Building the vit model with the following hyperparameters:\")\n                print(f\"Patch size: {learning_rate}\")\n                print(f\"num_patches: {num_patches}\") \n                print(f\"embed_dim: {embed_dim}\")\n                print(f\"num_heads: {num_heads}\")\n                print(f\"mlp_dim: {mlp_dim}\")\n                print(f\"num_layers: {num_layers}\")\n                print(f\"dropout_rate: {dropout_rate}\")\n                model = create_vit(input_shape, patch_size, num_patches, num_classes,\n                                embed_dim, num_heads, mlp_dim, num_layers, dropout_rate)\n                model.summary()\n                # Compile the model\n                model.compile(\n                    optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n            if model is not None:\n                \"Build VIT\"\n                # model.summary()\n            else:\n                print(\"Failed to build the VIT model.\")\n        except Exception as e:\n            print(f\"Error while building the VIT model: {e}\")\n        \n        print(\"Saving the data\")\n        fold_data = {\n\n            'X_train': X_train,\n            'X_val': X_val,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_val': y_val,\n            'y_test': y_test\n        }\n\n        with open(f\"/kaggle/working/CV_fold_data/fold_{fold}_data.pkl\", 'wb') as f:\n            pickle.dump(fold_data, f)\n        \n        print(\"Training the model\")\n        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n        y_pred = np.argmax(model.predict(X_test), axis=-1)\n        accuracy = accuracy_score(y_test, y_pred)\n        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n        \n            # Calculate precision, recall, and F1 score\n        precision = precision_score(y_test, y_pred, average='macro')\n        recall = recall_score(y_test, y_pred, average='macro')\n        f1 = f1_score(y_test, y_pred, average='macro')\n\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n\n        # Append the scores to the lists\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n        \n        # Append the losses and accuracies for each epoch to the respective lists\n        for epoch in range(epochs):\n            if fold == 1:\n                # For the first fold, initialize the lists with the first fold's values\n                mean_training_losses.append(history.history['loss'][epoch])\n                mean_training_accuracies.append(history.history['accuracy'][epoch])\n                mean_val_losses.append(history.history['val_loss'][epoch])\n                mean_val_accuracies.append(history.history['val_accuracy'][epoch])\n            else:\n                # For subsequent folds, add the fold's values to the running sum\n                mean_training_losses[epoch] += history.history['loss'][epoch]\n                mean_training_accuracies[epoch] += history.history['accuracy'][epoch]\n                mean_val_losses[epoch] += history.history['val_loss'][epoch]\n                mean_val_accuracies[epoch] += history.history['val_accuracy'][epoch]\n\n        # Save the model\n        if do_opt:\n            model_filename = f\"VIT_v{vit_version}_opt_{batch_size}_{epochs}_seed{seed_value}.h5\"\n        else:\n            model_filename = f\"VIT_v{vit_version}_{batch_size}_{epochs}_seed{seed_value}.h5\"\n        model_filepath = os.path.join(models_folder, f\"fold_{fold}_split_{n_splits}_{model_filename}\")\n        model.save(model_filepath)\n\n        # Save the results as a JSON file\n        if do_opt:\n            results_filename = f\"VIT_v{vit_version}_opt_{batch_size}_{epochs}_seed{seed_value}.json\"\n        else:\n            results_filename = f\"VIT_v{vit_version}_{batch_size}_{epochs}_seed{seed_value}.json\"\n        results_filepath = os.path.join(results_folder, f\"fold_{fold}_split_{n_splits}_{results_filename}\")\n\n        results = {\n            \"fold\": fold,\n            \"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_scores\": f1_scores,\n            \"test_loss\": test_loss,\n            \"test_accuracy\": test_accuracy,\n            \"training_loss\": history.history['loss'],\n            \"training_accuracy\": history.history['accuracy'],\n            \"validation_loss\": history.history['val_loss'],\n            \"validation_accuracy\": history.history['val_accuracy'],\n        }\n\n        with open(results_filepath, \"w\") as results_file:\n            json.dump(results, results_file, indent=4)\n        \n        # Append the results to the lists\n        accuracies.append(accuracy)\n        test_losses.append(test_loss)\n        test_accuracies.append(test_accuracy)\n        \n        print(f\"Fold {fold}, Accuracy: {accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n        print(\"=\" * 40)\n        fold += 1\n\n    # At the end, divide the sums by the number of folds to get the mean\n    mean_training_losses = [loss / n_splits for loss in mean_training_losses]\n    mean_training_accuracies = [acc / n_splits for acc in mean_training_accuracies]\n    mean_val_losses = [loss / n_splits for loss in mean_val_losses]\n    mean_val_accuracies = [acc / n_splits for acc in mean_val_accuracies]\n    # Convert lists to numpy arrays for easier calculation of mean and standard deviation\n    mean_accuracy = np.mean(accuracies)\n    mean_test_loss = np.mean(test_losses)\n    mean_test_accuracy = np.mean(test_accuracies)\n\n    # Calculate standard deviations\n    std_accuracy = np.std(accuracies)\n    std_test_loss = np.std(test_losses)\n    std_test_accuracy = np.std(test_accuracies)\n\n    std_training_loss = np.std(mean_training_losses)\n    std_training_accuracy = np.std(mean_training_accuracies)\n    std_val_loss = np.std(mean_val_losses)\n    std_val_accuracy = np.std(mean_val_accuracies)\n\n    precisions = np.array(precisions)\n    recalls = np.array(recalls)\n    f1_scores = np.array(f1_scores)\n\n      \n    print(f\"test losses: {test_losses}\")\n    print(f\"test accuracy: {test_accuracies}\")\n    \n    print(\"*\" * 40)\n    print(f\"Mean training loss over stratified {n_splits}-fold cross-validation: {np.mean(mean_training_losses):.4f}\")\n    print(f\"Mean training accuracy over stratified {n_splits}-fold cross-validation: {np.mean(mean_training_accuracies):.4f}\")\n    print(f\"Mean validation loss over stratified {n_splits}-fold cross-validation: {np.mean(mean_val_losses):.4f}\")\n    print(f\"Mean validation accuracy over stratified {n_splits}-fold cross-validation: {np.mean(mean_val_accuracies):.4f}\")\n    print(\"*\" * 40)\n    print(f\"Mean accuracy over stratified {n_splits}-fold cross-validation: {mean_accuracy:.4f}\")\n    print(f\"Mean test loss over stratified {n_splits}-fold cross-validation: {mean_test_loss:.4f}\")\n    print(f\"Mean test accuracy over stratified {n_splits}-fold cross-validation: {mean_test_accuracy:.4f}\")\n    print(\"*\" * 40)\n\n    print(f\"Standard deviation of training loss over stratified {n_splits}-fold cross-validation: {np.std(mean_training_losses):.4f}\")\n    print(f\"Standard deviation of training accuracy over stratified {n_splits}-fold cross-validation: {np.std(mean_training_accuracies):.4f}\")\n    print(f\"Standard deviation of validation loss over stratified {n_splits}-fold cross-validation: {np.std(mean_val_losses):.4f}\")\n    print(f\"Standard deviation of validation accuracy over stratified {n_splits}-fold cross-validation: {np.std(mean_val_accuracies):.4f}\")\n    print(\"*\" * 40)\n    print(f\"Standard deviation of accuracy over stratified {n_splits}-fold cross-validation: {std_accuracy:.4f}\")\n    print(f\"Standard deviation of test loss over stratified {n_splits}-fold cross-validation: {std_test_loss:.4f}\")\n    print(f\"Standard deviation of test accuracy over stratified {n_splits}-fold cross-validation: {std_test_accuracy:.4f}\")\n    print(\"*\" * 40)\n    \n    # Calculate and print the mean and standard deviation of precision, recall, and F1 score\n    print(f\"Mean precision over stratified {n_splits}-fold cross-validation: {np.mean(precisions):.4f}\")\n    print(f\"Standard deviation of precision over stratified {n_splits}-fold cross-validation: {np.std(precisions):.4f}\")\n    print(f\"Mean recall over stratified {n_splits}-fold cross-validation: {np.mean(recalls):.4f}\")\n    print(f\"Standard deviation of recall over stratified {n_splits}-fold cross-validation: {np.std(recalls):.4f}\")\n    print(f\"Mean F1 score over stratified {n_splits}-fold cross-validation: {np.mean(f1_scores):.4f}\")\n    print(f\"Standard deviation of F1 score over stratified {n_splits}-fold cross-validation: {np.std(f1_scores):.4f}\")\n\n    \n# Define the directory where the JSON files are stored\n\n    fig, axes = plt.subplots(2, n_splits, figsize=(20, 10))\n\n    for fold in range(1, n_splits+1):\n        # Load the JSON file for this fold\n        if do_opt:\n            cnn_filename = f\"VIT_v{vit_version}_opt_{batch_size}_{epochs}_seed{seed_value}.json\"\n        else:\n            cnn_filename = f\"VIT_v{vit_version}_{batch_size}_{epochs}_seed{seed_value}.json\"\n        results_filename = f\"fold_{fold}_split_{n_splits}_{cnn_filename}\"\n        \n        results_filepath = os.path.join(results_folder, results_filename)\n        \n        with open(results_filepath, \"r\") as results_file:\n            results = json.load(results_file)\n        \n        # Plot training and validation loss\n        axes[0, fold-1].plot(results[\"training_loss\"], label='Train')\n        axes[0, fold-1].plot(results[\"validation_loss\"], label='Validation')\n        axes[0, fold-1].set_title(f'Fold {fold} Loss')\n        axes[0, fold-1].set_xlabel('Epochs')\n        axes[0, fold-1].set_ylabel('Loss')\n        axes[0, fold-1].legend()\n\n        # Plot training and validation accuracy\n        axes[1, fold-1].plot(results[\"training_accuracy\"], label='Train')\n        axes[1, fold-1].plot(results[\"validation_accuracy\"], label='Validation')\n        axes[1, fold-1].set_title(f'Fold {fold} Accuracy')\n        axes[1, fold-1].set_xlabel('Epochs')\n        axes[1, fold-1].set_ylabel('Accuracy')\n        axes[1, fold-1].legend()\n\n    plt.tight_layout()\n    plt.show()\n    \n    return mean_val_accuracies  ","metadata":{"execution":{"iopub.status.busy":"2023-10-05T20:18:53.815479Z","iopub.execute_input":"2023-10-05T20:18:53.815832Z","iopub.status.idle":"2023-10-05T20:18:53.845707Z","shell.execute_reply.started":"2023-10-05T20:18:53.815800Z","shell.execute_reply":"2023-10-05T20:18:53.844761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data\ndata_file = \"../Data/HC05_HC07.csv\"\n\nif OPT_DL:\n    X_filtered, y = preprocess_cv_raw_data(data_file)\n    optimize_hyperparameters(X_filtered, y, 50, dl_cv_models_folder, dl_cv_results_folder)\n\nelse:\n    # Train deep learning model with cross validation\n    if CV_DL:\n        print(\"CV preprocess\")\n        X_filtered, y = preprocess_cv_raw_data(data_file)\n    else:\n        # Preprocess the raw data\n        print(\"Preprocess raw data\")\n        X_train, X_test, y_train, y_test = preprocess_raw_data(\n            data_file, TEST_SET)\n\n# Do Deep Learning\nif DO_DL:\n    dl_weights_path = \"../During_train/cnn_v3-250-val_acc0.86.h5\"\n    if os.path.exists(dl_model_path):\n        if DO_TRANSFORMER_VISION:\n            model = tf.keras.models.load_model(\n                dl_model_path, custom_objects={'ClassToken': ClassToken, 'TransformerBlock': TransformerBlock})\n        elif DO_CNN:\n            model = tf.keras.models.load_model(dl_model_path)\n            print(\"Loaded trained model from\", dl_model_path)\n    elif os.path.exists(dl_weights_path):\n        if DO_TRANSFORMER_VISION:\n            print(\"hello\")\n            # model = create_vit(input_shape, patch_size, num_patches, num_classes,\n            #                         embed_dim, num_heads, mlp_dim, num_layers, dropout_rate)\n            # model.load_weights(dl_weights_path)\n        elif DO_CNN:\n            X_train = X_train.reshape(\n                X_train.shape[0], X_train.shape[1], 1)\n            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n            input_shape = (X_train.shape[1], 1)\n            num_classes = len(np.unique(y_train))\n            learning_rate = 1.4286522423518213e-05\n            dropout_rate = 0.2\n            l2_regularizer = 0.000417822262290105\n            model = cnn_model_3_opt(\n                input_shape, num_classes, learning_rate, dropout_rate, l2_regularizer)\n            model.load_weights(dl_weights_path)\n            print(\"Loaded trained model weights from\", dl_weights_path)\n            # Evaluating the model\n            print(\"Evaluating the CNN model... \")\n            # Calculate model loss and accuracy\n            test_loss, test_accuracy = model.evaluate(X_test, y_test)\n            print(\"Test loss:\", test_loss)\n            print(\"Test accuracy:\", test_accuracy)\n    else:\n        if CV_DL:\n            # learning_rate = 3.408170980489466e-05\n            # dropout_rate = 0.3\n            # l2_regularizer = 0.00010124894257897855\n            if DO_CNN:\n                learning_rate = 1.4286522423518213e-05\n                dropout_rate = 0.2\n                l2_regularizer = 0.000417822262290105\n                # Start the timer\n                start_time = time.time()\n                perform_cross_validation(X_filtered, y, learning_rate, dropout_rate, l2_regularizer,\n                                        n_splits=DL_CV_FOLD, epochs=DL_EPOCH, batch_size=DL_BATCH_SIZE, cnn_version=DL_CNN_VERSION,\n                                        do_opt=True, models_folder=dl_cv_models_folder, results_folder=dl_cv_results_folder)\n                # End the timer\n                end_time = time.time()\n                # Calculate the elapsed time\n                elapsed_time = end_time - start_time\n                # Print the elapsed time\n                print(\"Training time: {:.2f} seconds\".format(elapsed_time))\n            elif DO_TRANSFORMER_VISION:\n                # Train and evaluate deep learning models\n                print(\"Building Transformer model...\")\n                patch_size = 30\n                embed_dim = 64\n                num_heads = 4\n                mlp_dim = 128  # patch_size * patch_size * input_shape[1]\n                num_layers = 4\n                dropout_rate = 0.3\n\n                # Create the model\n                perform_vit_cross_validation(X_filtered, y, patch_size, embed_dim, num_heads, mlp_dim, num_layers, dropout_rate,\n                                            n_splits=DL_CV_FOLD, epochs=DL_EPOCH, batch_size=DL_BATCH_SIZE, \n                                            vit_version = DL_TRANSFORMER_VISION_VERSION, val_set = 0.2,\n                                            do_opt = True, models_folder = \"/kaggle/working/CV_VIT_models\", results_folder = \"/kaggle/working/CV_VIT_results\")\n\n        else:\n            # Reshape the input data for deep learning models\n            print(\"Reshaping the data for CNN model...\")\n            X_train = X_train.reshape(\n                X_train.shape[0], X_train.shape[1], 1)\n            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n            input_shape = (X_train.shape[1], 1)\n            num_classes = len(np.unique(y_train))\n\n            # Splitting the training data to create a validation set\n            print(\"Splitting the training data to create a validation set...\")\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train, y_train, test_size=VAL_SET, random_state=SEED, stratify=y_train)\n\n            filepath = \"\"\n\n            if DO_CNN:\n                # Train and evaluate deep learning models\n                print(\"Building CNN model...\")\n                # model = cnn_model_3_3(input_shape, num_classes)\n                # learning_rate = 3.408170980489466e-05\n                # dropout_rate = 0.3\n                # l2_regularizer = 0.00010124894257897855\n                learning_rate = 1.4286522423518213e-05\n                dropout_rate = 0.2\n                l2_regularizer = 0.000417822262290105\n                model = cnn_model_3_opt(\n                    input_shape, num_classes, learning_rate, dropout_rate, l2_regularizer)\n\n                filepath = '../During_train/cnn_v3-{epoch:02d}-val_acc{val_accuracy:.2f}.h5'\n\n            elif DO_TRANSFORMER_VISION:\n                # Train and evaluate deep learning models\n                print(\"Building Transformer model...\")\n                # Define model parameters\n                print(f\"X_train shape = {X_train.shape}\")\n                input_shape = (X_train.shape[1], 1)  # (270,1)\n                # cancer cell lines, monocytes, and T-cells\n                num_classes = len(np.unique(y_train))\n                patch_size = 3\n                num_patches = input_shape[0] // patch_size\n                embed_dim = 64\n                num_heads = 4\n                mlp_dim = 128  # patch_size * patch_size * input_shape[1]\n                num_layers = 4\n                dropout_rate = 0.3\n\n                # Create the model\n                model = create_vit(input_shape, patch_size, num_patches, num_classes,\n                                   embed_dim, num_heads, mlp_dim, num_layers, dropout_rate)\n                model.summary()\n                # Compile the model\n                model.compile(\n                    optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n                filepath = '../During_train/vit_v11-{epoch:02d}-val_acc{val_accuracy:.2f}.h5'\n\n            # Train the model\n            print(\"Training and evaluating the model...\")\n            # Start the timer\n            start_time = time.time()\n            model_history = model.fit(X_train, y_train, batch_size=DL_BATCH_SIZE,\n                                      epochs=DL_EPOCH, validation_data=(X_val, y_val), shuffle=True, callbacks=callbacks)\n            # End the timer\n            end_time = time.time()\n            # Calculate the elapsed time\n            elapsed_time = end_time - start_time\n            # Print the elapsed time\n            print(\"Training time: {:.2f} seconds\".format(elapsed_time))\n            # Compile the model\n\n            # Save the deep learning model\n            print(\"Saving model to \", dl_model_path)\n            model.save(dl_model_path)\n\n            # Save the deep learning model's weights\n            print(\"Saving model weights to \", dl_weights_path)\n            model.save_weights(dl_weights_path)\n\n            # Evaluating the model\n            print(\"Evaluating the CNN model... \")\n            # Calculate model loss and accuracy\n            test_loss, test_accuracy = model.evaluate(X_test, y_test)\n            print(\"Test loss:\", test_loss)\n            print(\"Test accuracy:\", test_accuracy)\n\n            # Save the model's metrics\n            print(\"Saving the model metrics to \", dl_metrics_path)\n            save_dl_metrics(model_history, test_loss,\n                            test_accuracy, dl_metrics_path)\n\n            # Plot the model history\n            plot_dl_history(model_history)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T20:18:59.960351Z","iopub.execute_input":"2023-10-05T20:18:59.960677Z","iopub.status.idle":"2023-10-05T20:55:08.338661Z","shell.execute_reply.started":"2023-10-05T20:18:59.960650Z","shell.execute_reply":"2023-10-05T20:55:08.337783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data\ndata_file = \"../Data/HC05_HC07.csv\"\n\nCV_DL = False\n\nif OPT_DL:\n    X_filtered, y = preprocess_cv_raw_data(data_file)\n    optimize_hyperparameters(X_filtered, y, 50, dl_cv_models_folder, dl_cv_results_folder)\n\nelse:\n    # Train deep learning model with cross validation\n    if CV_DL:\n        X_filtered, y = preprocess_cv_raw_data(data_file)\n    else:\n        # Preprocess the raw data\n        X_train, X_test, y_train, y_test = preprocess_raw_data(\n            data_file, TEST_SET)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T20:55:08.340156Z","iopub.execute_input":"2023-10-05T20:55:08.340449Z","iopub.status.idle":"2023-10-05T20:55:10.053672Z","shell.execute_reply.started":"2023-10-05T20:55:08.340423Z","shell.execute_reply":"2023-10-05T20:55:10.052628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit_model_path = \"/kaggle/working/CV_VIT_models/\"\ndata_model_path = \"/kaggle/working/CV_fold_data/\"\ncm_model_path = \"/kaggle/working/CV_VIT_CM/\"\n\n# Lists to store metrics\naccuracies = []\nprecisions = []\nrecalls = []\nf1_scores = []\nconfusion_matrices_vit = []\n\n# Loop through each fold\nfor fold in range(1, 6):\n    # Load datasets for this fold\n    with open(f\"{data_model_path}fold_{fold}_data.pkl\", 'rb') as f:\n        fold_data = pickle.load(f)\n        \n    X_test = fold_data['X_test']\n    y_test = fold_data['y_test']\n    \n    # Load the model for this fold\n    model_file = f\"fold_{fold}_split_5_VIT_v14_opt_32_250_seed7.h5\"\n    model_path = os.path.join(vit_model_path, model_file)\n    model = tf.keras.models.load_model(model_path, custom_objects={'ClassToken': ClassToken, 'TransformerBlock': TransformerBlock})\n    \n    # Predict and evaluate\n    y_pred = np.argmax(model.predict(X_test), axis=-1)\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='macro')\n    recall = recall_score(y_test, y_pred, average='macro')\n    f1 = f1_score(y_test, y_pred, average='macro')\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # Save the confusion matrix\n    with open(f\"{cm_model_path}VIT_cm_fold_{fold}.pkl\", 'wb') as cm_file:\n        pickle.dump(cm, cm_file)\n    \n    # Store metrics\n    accuracies.append(accuracy)\n    precisions.append(precision)\n    recalls.append(recall)\n    f1_scores.append(f1)\n    confusion_matrices_vit.append(cm)\n    \n    # Print metrics for this fold\n    print(f\"Fold {fold}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"-\" * 40)\n\nconfusion_matrices_vit","metadata":{"execution":{"iopub.status.busy":"2023-10-05T20:55:10.055288Z","iopub.execute_input":"2023-10-05T20:55:10.055826Z","iopub.status.idle":"2023-10-05T20:55:10.207079Z","shell.execute_reply.started":"2023-10-05T20:55:10.055786Z","shell.execute_reply":"2023-10-05T20:55:10.205754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r CV_VIT_models.zip /kaggle/working/CV_VIT_models/","metadata":{"execution":{"iopub.status.busy":"2023-10-01T18:17:19.067959Z","iopub.execute_input":"2023-10-01T18:17:19.068323Z","iopub.status.idle":"2023-10-01T18:17:20.175371Z","shell.execute_reply.started":"2023-10-01T18:17:19.068293Z","shell.execute_reply":"2023-10-01T18:17:20.17419Z"},"trusted":true},"execution_count":null,"outputs":[]}]}